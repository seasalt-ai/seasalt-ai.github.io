const e=`---
title: "จากเดโมสู่ความสำเร็จ: ก้าวข้ามอัลกอริทึมเสียงของการประชุมยุคใหม่ (3/5)"
metatitle: "เดโมสู่ความสำเร็จ (3/5): ก้าวข้ามอัลกอริทึมเสียง"
date: 2021-07-30 17:43:38-07:00
modified_date: 2025-07-29 00:00:00+00:00
draft: false
author: Cody Kim, Shayne Mei
description: "ในตอนที่สามของซีรีส์บล็อกนี้ ติดตามการเดินทางของ Seasalt.ai สู่การสร้าง SeaMeet โซลูชันการประชุมยุคใหม่แบบร่วมมือ"
weight: 1
tags:
  - SeaMeet
image: images/blog/3-implementing-Microsoft-modern-meetings-and-beyond/SeaMeet animation.gif
canonicalURL: /blog/seameet-voice-speech-recognition/
url: /blog/seameet-voice-speech-recognition/
---

*ตลอดซีรีส์บล็อกนี้ ติดตามการเดินทางของ Seasalt.ai สู่การสร้างประสบการณ์การประชุมยุคใหม่ที่สมบูรณ์แบบ ตั้งแต่จุดเริ่มต้นเล็ก ๆ ไปจนถึงการปรับแต่งบริการบนฮาร์ดแวร์และโมเดลต่าง ๆ การผสานระบบ NLP ที่ล้ำสมัย และสุดท้ายคือการสร้าง SeaMeet โซลูชันการประชุมยุคใหม่แบบร่วมมือ*

## ก้าวข้ามอัลกอริทึม
Modern Meetings เป็นเดโมที่ยอดเยี่ยม แต่ยังคงเป็นเดโม ยังมีทางอีกไกลกว่าจะพร้อมใช้งานจริง เราได้ [นำไปใช้สำเร็จ](https://seasalt.ai/blog/5-seameet-voice-transcription-speech-recognition-algorithm/) เวอร์ชันเดโมด้วย Microsoft Azure stack แต่เมื่อพบข้อบกพร่องของซอฟต์แวร์ เราจึงตัดสินใจเปลี่ยนอัลกอริทึมเป็นของเราเอง และทำให้ประสบการณ์ทั้งหมดราบรื่น เบา และยืดหยุ่นมากขึ้น
Modern Meetings มีองค์ประกอบหลัก 4 ส่วน:

1. การประมวลผลสัญญาณบนไมโครโฟนอาเรย์ โดยเฉพาะ beam forming
2. การแยกและระบุผู้พูด
3. การรู้จำเสียงพูดแบบปรับแต่ง
4. UI ที่ดีกว่า

ต่อไปเราจะอธิบายรายละเอียดของแต่ละองค์ประกอบสำคัญ

<center>
<img src="/images/blog/5-seameet-voice-intelligence-meeting-transcription-speech-recognition-algorithm-of-modern-meeting/tech-stack.png" alt="SeaMeet architect with 4 major components"/>

*เราได้ปรับแต่งทั้ง 4 องค์ประกอบหลักของ Modern Meetings ด้วยเทคโนโลยีของเราเอง: 1. การประมวลผลสัญญาณด้วยไมโครโฟนอาเรย์; 2. การแยกและระบุผู้พูด; 3. การรู้จำเสียงพูดแบบปรับแต่ง; 4. UI เว็บสมัยใหม่*
</center>

### การประมวลผลสัญญาณบนไมโครโฟนอาเรย์
ไมโครโฟนอาเรย์เมื่อเทียบกับไมโครโฟนเดี่ยวแบบใกล้ปาก สามารถรับเสียงจากทุกทิศทาง 360 องศา ในระยะสูงสุด 5 เมตร ดังนั้นไมโครโฟนอาเรย์หนึ่งชุดสามารถเก็บเสียงในห้องประชุมขนาดกลาง 10x10 เมตรได้ เนื่องจากไมโครโฟนทั้งหมดรวมอยู่ในอุปกรณ์เดียว จึงช่วยลดสายไฟในห้องประชุมและทำให้การติดตั้งและบำรุงรักษาง่ายขึ้น
เป้าหมายของการใช้ไมโครโฟนอาเรย์คือการให้ข้อมูลคุณภาพดีที่สุดแก่โมเดลของเรา ก่อนเข้าสู่การรู้จำเสียงพูดอัตโนมัติ เราจะดำเนินการอัลกอริทึมการประมวลผลสัญญาณหลายอย่าง ส่วนสำคัญของกระบวนการล่วงหน้าคืออัลกอริทึม beamforming เนื่องจากเราใช้ไมโครโฟนอาเรย์แบบวงกลมหลายตัว เราจึงสามารถใช้ความแตกต่างของเวลาที่เสียงเดินทางถึงไมโครโฟนแต่ละตัวได้ Beamforming จะกำหนดลักษณะสำคัญของสัญญาณ — beam ที่ดีที่สุด — และขยายความถี่เหล่านี้ในขณะที่ลดเสียงรบกวนที่ไม่ต้องการ ผลลัพธ์คือการลดเสียงรบกวนและเสียงสะท้อน ในขณะที่สัญญาณหลัก (เสียงพูด) ดังและชัดเจนขึ้น

สำหรับประสิทธิภาพสูงสุดของอัลกอริทึม beamforming หลายตัว จำเป็นต้องรู้ตำแหน่งที่แน่นอนของแหล่งเสียง (ผู้พูด) เมื่อเทียบกับไมโครโฟน แต่ในแอปพลิเคชันจริงเป็นไปไม่ได้ เราจึงคำนวณน้ำหนักฟิลด์ไกลโดยกำหนดทิศทางของแหล่งเสียง ขั้นตอนแรกนี้เรียกว่า source localization หรือโดยเฉพาะ Direction of Arrival (DOA) ซึ่งพิสูจน์แล้วว่ายาก ปัญหาหลักคือการทำให้ผลลัพธ์ราบรื่น อัลกอริทึมให้ผลลัพธ์ประมาณถูกต้อง แต่แหล่งเสียงที่กำหนดจะเปลี่ยนไปมา 30 องศาทั้งสองข้างของทิศทางจริง ซึ่งส่งผลต่อ beamforming วิธีแก้ไขของเราคือให้ source localization ใช้เฉพาะช่วงความถี่ที่เข้ารหัสเสียงพูดของมนุษย์เป็นหลัก ร่วมกับเทคนิคการทำให้ราบรื่นโดยเก็บ "ประวัติ" DOA เพื่อหาค่าเฉลี่ย เมื่อได้ผลลัพธ์ DOA ที่เชื่อถือได้มากขึ้น เราจึงคำนวณน้ำหนักฟิลด์ไกลและกำหนด beam ที่ดีที่สุด

ด้วยชุดอัลกอริทึมบน Kinect DK: beamforming, การลดเสียงรบกวน, การลดเสียงสะท้อน, source localization เราสามารถสร้างเสียงพูดของมนุษย์ที่ชัดเจนและเพิ่มคุณภาพแบบเรียลไทม์ พร้อมระบุทิศทางของผู้พูดโดยประมาณ ซึ่งช่วยในการระบุผู้พูดในขั้นตอนถัดไป

### การแยกและระบุผู้พูด

องค์ประกอบถัดไปของระบบถอดเสียงการประชุมยุคใหม่คือการรู้จำผู้พูดอัตโนมัติ ตามที่กล่าวไว้ในตอนก่อน การอ่านข้อความสนทนาที่ไม่มีข้อมูลผู้พูดนั้นน่าหงุดหงิดและขัดกับวัตถุประสงค์ของระบบ นี่คือจุดที่การรู้จำผู้พูดมีบทบาท

ด้วยองค์ประกอบนี้ เราสามารถจัดตำแหน่งถอดเสียงและเสียงกับชื่อผู้พูดโดยอัตโนมัติ ในการดำเนินการนี้ เราใช้กระบวนการที่เรียกว่า diarization ซึ่งจัดกลุ่มส่วนเสียงเป็นจำนวนกลุ่มตามจำนวนผู้พูดในบันทึกเสียง วิธีนี้ใช้ระบบตรวจจับกิจกรรมเสียง (VAD) เพื่อกำหนดส่วนเสียง จากนั้นเราสามารถดึงเวกเตอร์แทนหน้าต่างสั้น ๆ ได้ เวกเตอร์แต่ละตัวที่ดึงจากหน้าต่างเรียกว่า utterance-level xvector และเมื่อเฉลี่ยจะได้ speaker-level xvector เวกเตอร์เหล่านี้จะถูกนำไปผ่านอัลกอริทึมการจัดกลุ่ม โดยแต่ละกลุ่มแสดงถึงส่วนเสียงที่เป็นของผู้พูดเดียวกัน การเลือกอัลกอริทึมการจัดกลุ่มมีผลต่อประสิทธิภาพ diarization อย่างมาก เราได้อัตราความผิดพลาดในการแยกผู้พูด (DER) ที่เหมาะสมด้วยการจัดกลุ่มแบบสเปกตรัมโดยใช้เมทริกซ์ affinity ที่ปรับอัตโนมัติด้วยค่า Normalized Maximum Eigengap (NME) สุดท้ายต้องตัดสินใจว่าผู้พูดแต่ละกลุ่มคือใคร ก่อนการประชุมสามารถทำ enrollment เพื่อดึง xvector จากการบันทึกเสียง 40 วินาทีของแต่ละผู้พูด ซึ่งสามารถเปรียบเทียบกับกลุ่มเพื่อระบุผู้พูดที่เกี่ยวข้อง

จุดเด่นของกระบวนการนี้คือความยืดหยุ่น สำหรับหลายสถานการณ์การประชุม การมีบันทึกเสียงของแต่ละผู้พูดล่วงหน้าไม่ใช่เรื่องง่าย เช่น การประชุมธุรกิจกับลูกค้า VIP หรือสัมมนาขนาดใหญ่ที่มีผู้พูด 50 คน ในกรณีนี้ แม้จะข้ามขั้นตอน enrollment ระบบ diarization ของเรายังสามารถจัดกลุ่มส่วนเสียงและรวมกลุ่มที่เป็นของผู้พูดเดียวกันได้ เพียงให้มนุษย์ฟังไม่กี่วินาทีจากแต่ละกลุ่มเพื่อระบุผู้พูด ร่วมกับ UI สมัยใหม่โดยเฉพาะ เราสามารถให้ฟังก์ชันเดียวกันแต่มีความยืดหยุ่นมากขึ้น

### การรู้จำเสียงพูดแบบปรับแต่ง

หลังจากได้รู้จัก Microsoft meeting transcriber และจุดเด่นของมัน เราก็พร้อมที่จะทำให้ระบบของเราเป็นอิสระอย่างสมบูรณ์และก้าวข้ามผลิตภัณฑ์ที่ปฏิวัติวงการ แรงขับเคลื่อนของ Modern Meetings และทุกผลิตภัณฑ์ถอดเสียงคือโมเดลการรู้จำเสียงพูดอัตโนมัติ (ASR) ดังนั้นเราจึงให้ความสำคัญกับส่วนนี้มากที่สุด
Azure Cognitive Services มีโมเดลให้เลือกมากมายสำหรับหลายภาษาและสำเนียง อย่างไรก็ตาม ประสิทธิภาพระหว่างสำเนียงต่าง ๆ นั้นแยกแยะได้ยาก สำหรับสำเนียงภาษาอังกฤษแต่ละแบบ อาจใช้ความพยายามและข้อมูลส่วนใหญ่กับโมเดลภาษาอังกฤษสหรัฐฯ แล้วจึงปรับแต่งด้วยข้อมูลสำเนียงเพื่อสร้างโมเดลสำเนียงต่าง ๆ เราต้องการให้แน่ใจว่าหากมีโมเดลเฉพาะ ต้องปรับแต่งให้เหมาะกับกรณีใช้งานเฉพาะ ซึ่งหมายถึงการรวบรวมข้อมูลเสียงและถอดเสียงในพื้นที่หลายพันชั่วโมง และใช้เวลาหลายสัปดาห์ในการฝึกและปรับแต่ง แต่ความพึงพอใจเมื่อเห็นโมเดลดีขึ้นทุก epoch และทำตามที่สัญญาไว้ก็คุ้มค่า

เมื่อมีโมเดลพื้นฐานที่แข็งแกร่งแล้ว ขั้นตอนต่อไปคือขยายการใช้งานและความสามารถในการปรับแต่ง ทุกอุตสาหกรรมมีศัพท์เฉพาะมากมาย ทำให้โมเดล ASR แยกแยะคำศัพท์เฉพาะกับคำทั่วไปที่ออกเสียงคล้ายกันได้ยาก

คำตอบของเราคือ SeaVoice ซึ่งให้แพลตฟอร์มศูนย์กลางที่ผู้ใช้สามารถปรับแต่งโมเดลให้ตรงกับความต้องการเฉพาะได้ง่าย
`;export{e as default};

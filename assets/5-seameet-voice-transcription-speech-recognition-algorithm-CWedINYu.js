const e=`---
title: "از دمو تا موفقیت: پیشرفت‌های الگوریتم‌های صوتی جلسات مدرن (۳/۵)"
metatitle: "از دمو تا موفقیت (۳/۵): پیشرفت‌های الگوریتم‌های صوتی جلسات مدرن"
date: 2021-07-30 17:43:38-07:00
modified_date: 2025-07-29 00:00:00+00:00
draft: false
author: Cody Kim, Shayne Mei
description: "در بخش سوم این سری وبلاگ، سفر Seasalt.ai در ایجاد راه‌حل جلسات مدرن SeaMeet را دنبال کنید."
weight: 1
tags:
  - SeaMeet
image: images/blog/3-implementing-Microsoft-modern-meetings-and-beyond/SeaMeet animation.gif
canonicalURL: /blog/seameet-voice-speech-recognition/
url: /blog/seameet-voice-speech-recognition/
---

در این سری وبلاگ، سفر Seasalt.ai در ایجاد تجربه کامل جلسات مدرن را دنبال کنید، از شروع اولیه، تا بهینه‌سازی روی سخت‌افزارها و مدل‌های مختلف، تا ادغام پیشرفته‌ترین سیستم‌های NLP، و در نهایت تحقق راه‌حل جلسات مدرن SeaMeet.

## فراتر از الگوریتم
Modern Meetings یک دمو عالی بود، اما همیشه در مرحله دمو باقی ماند. راه طولانی‌ای برای تبدیل آن به محصول واقعی وجود داشت. ما ابتدا با موفقیت نسخه دمو را با استفاده از پشته فناوری Microsoft Azure پیاده‌سازی کردیم. اما پس از شناخت نقایص مختلف نرم‌افزار، تصمیم گرفتیم الگوریتم‌های خودمان را جایگزین کنیم و کل تجربه را روان‌تر، سبک‌تر و انعطاف‌پذیرتر کنیم.

Modern Meetings دارای چهار جزء اصلی است:

۱. پردازش سیگنال آرایه میکروفون، به ویژه beamforming
۲. جداسازی و شناسایی گوینده
۳. تشخیص گفتار سفارشی
۴. رابط کاربری بهتر

در ادامه جزئیات تمام اجزای مهم را بررسی خواهیم کرد.

<center>
<img src="/images/blog/5-seameet-voice-intelligence-meeting-transcription-speech-recognition-algorithm-of-modern-meeting/tech-stack.png" alt="SeaMeet architect with 4 major components"/>

ما چهار جزء اصلی Modern Meetings را با پشته فناوری خودمان بازسازی کردیم: ۱. پردازش سیگنال آرایه میکروفون؛ ۲. جداسازی و شناسایی گوینده؛ ۳. تشخیص گفتار سفارشی؛ ۴. رابط وب مدرن.
</center>

### پردازش سیگنال آرایه میکروفون
در مقایسه با میکروفون تک نزدیک، آرایه میکروفون می‌تواند صدا را از محدوده ۳۶۰ درجه و تا فاصله ۵ متری دریافت کند. بنابراین، یک آرایه میکروفون می‌تواند صدا را در یک اتاق جلسه متوسط ۱۰ متر × ۱۰ متر جمع‌آوری کند. تمام میکروفون‌ها در یک دستگاه متمرکز شده‌اند، که به طور قابل توجهی کابل‌های اتاق جلسه را کاهش می‌دهد و نصب و نگهداری را ساده می‌کند.

از سوی دیگر، هدف نهایی استفاده از آرایه میکروفون ارائه بهترین کیفیت داده به مدل‌های ما است. بنابراین، قبل از تشخیص خودکار گفتار، ما چندین الگوریتم پردازش سیگنال را اجرا می‌کنیم. هسته خط لوله پیش‌پردازش الگوریتم beamforming است. از آنجا که ما از آرایه میکروفون دایره‌ای چندگانه استفاده می‌کنیم، می‌توانیم از تفاوت‌های زمانی کوچک رسیدن صدا به هر میکروفون استفاده کنیم. beamforming برای تعیین ویژگی‌های اصلی سیگنال (بهترین beam) عمل می‌کند، این فرکانس‌ها را تقویت می‌کند و در عین حال صداهای ناخواسته را تضعیف می‌کند. نتیجه کاهش نویز و dereverberation است، که سیگنال اصلی (گفتار) را بلندتر و واضح‌تر می‌کند.

بهترین عملکرد بسیاری از الگوریتم‌های beamforming نیاز به دانستن موقعیت منبع صدا (گوینده) نسبت به میکروفون دارد. اما این در کاربردهای واقعی تقریباً غیرممکن است، بنابراین ما ابتدا با تعیین جهت منبع صدا، وزن‌های far-field را محاسبه می‌کنیم. این مرحله localization منبع صدا نامیده می‌شود، یا به طور خاص‌تر direction of arrival (DOA). مشکل اصلی که با آن مواجه شدیم smoothness بود. الگوریتم می‌تواند تقریباً نتایج صحیح بدهد، اما منبع صدا تعیین شده در محدوده ۳۰ درجه در هر دو طرف جهت واقعی نوسان می‌کند، که بر beamforming تأثیر می‌گذارد. راه‌حل ما این بود که فقط به الگوریتم localization منبع صدا اجازه دهیم از باندهای فرکانسی استفاده کند که فرکانس‌های اصلی گفتار انسان را رمزگذاری می‌کنند، و تکنیک‌های smoothing را ترکیب کنیم، "تاریخچه" نتایج DOA را برای میانگین‌گیری حفظ کنیم. نتایج DOA قابل اعتمادتر به ما اجازه می‌دهد وزن‌های far-field را محاسبه کنیم و بهترین beam را تعیین کنیم.

سری الگوریتم‌های اجرا شده روی Kinect DK: beamforming، کاهش نویز، dereverberation، localization منبع صدا، به ما اجازه می‌دهد گفتار انسان واضح و تقویت شده را در زمان واقعی تولید کنیم و تقریباً جهت گوینده را تعیین کنیم. این به طور قابل توجهی به مرحله بعدی شناسایی گوینده کمک خواهد کرد.

### جداسازی و شناسایی گوینده

جزء کلیدی بعدی سیستم رونویسی جلسه، شناسایی خودکار گوینده است. همانطور که در بخش قبلی این سری ذکر شد، خواندن متن گفتگو بدون اطلاعات گوینده ناامیدکننده است و کاملاً هدف سیستم را از دست می‌دهد. اینجاست که شناسایی گوینده وارد می‌شود.

از طریق این جزء، ما می‌توانیم به طور خودکار رونویسی و صدا را با نام‌های گوینده تراز کنیم. روش پیاده‌سازی استفاده از تکنیک‌های جداسازی است که قطعات صوتی را به گروه‌هایی خوشه‌بندی می‌کند که با تعداد گویندگان در ضبط مطابقت دارد. این روش از سیستم تشخیص فعالیت صوتی (VAD) برای تعیین قطعات گفتار استفاده می‌کند، که از آن نمایش‌های برداری پنجره‌های کوتاه استخراج می‌شود. بردار استخراج شده از هر پنجره utterance-level xvector نامیده می‌شود، که پس از میانگین‌گیری، speaker-level xvector را تولید می‌کند. سپس این xvectorها خوشه‌بندی می‌شوند، هر خوشه قطعات صوتی متعلق به همان گوینده را نشان می‌دهد. قابل ذکر است که انتخاب الگوریتم خوشه‌بندی به طور قابل توجهی بر عملکرد جداسازی تأثیر می‌گذارد، و ما با استفاده از الگوریتم spectral clustering با ماتریس affinity آستانه و ترکیب با مقدار normalized maximum eigengap (NME) برای تنظیم خودکار، بهترین نرخ خطای جداسازی (DER) را به دست آوردیم. در نهایت، باید تعیین شود که هر خوشه چه گوینده‌ای را نشان می‌دهد. قبل از جلسه، می‌توان xvector هر گوینده را از ۴۰ ثانیه ضبط استخراج کرد و با نتایج خوشه‌بندی مقایسه کرد تا گوینده مربوطه شناسایی شود.

انعطاف‌پذیری این فرآیند در این است که: در بسیاری از سناریوهای جلسه، به دست آوردن ضبط هر گوینده از قبل واقع‌بینانه نیست. مثلاً جلسات تجاری با مشتریان VIP یا سمینارهای بزرگ با ۵۰ سخنران. در چنین مواردی، با رد کردن مرحله ثبت‌نام، سیستم جداسازی ما همچنان می‌تواند قطعات صوتی را گروه‌بندی کند. فقط نیاز به استخراج چند ثانیه صدا از هر خوشه توسط انسان برای تعیین هویت گوینده است. همراه با رابط کاربری مدرن اختصاصی، می‌تواند همان عملکرد را با انعطاف‌پذیری بیشتر ارائه دهد.

### تشخیص گفتار سفارشی

پس از شناخت قدرت Microsoft Meeting Transcriber، ما آماده بودیم سیستم را کاملاً مستقل کنیم و فراتر از محصول انقلابی موجود برویم. هسته Modern Meetings و هر محصول رونویسی، مدل تشخیص خودکار گفتار (ASR) است، بنابراین ما بیشترین توجه را به آن اختصاص دادیم.

Azure Cognitive Services مدل‌هایی برای زبان‌ها و گویش‌های مختلف ارائه می‌دهد، اما تمایز عملکرد بین گویش‌های مختلف دشوار است. برای گویش‌های مختلف انگلیسی، احتمالاً بیشترین تلاش و داده روی مدل انگلیسی آمریکایی متمرکز شده، سپس با داده‌های لهجه‌دار برای تولید مدل‌های گویش مختلف fine-tune شده است. ما می‌خواستیم اطمینان حاصل کنیم که اگر مدل مستقل ارائه می‌دهیم، باید برای موارد استفاده خاص تنظیم شده باشد. این به معنای جمع‌آوری هزاران ساعت صدا و رونویسی محلی‌سازی شده و صرف هفته‌ها برای آموزش و fine-tuning است. اما دیدن پیشرفت مدل در هر epoch و تحقق وعده‌ها بسیار رضایت‌بخش است.

با داشتن مدل پایه محکم، مرحله بعدی گسترش قابلیت استفاده و سفارشی‌سازی است. هر صنعتی دارای اصطلاحات اختصاصی زیادی است که تشخیص کلمات نادر از کلمات رایج و مشابه در تلفظ را برای مدل‌های ASR دشوار می‌کند.

راه‌حل ما SeaVoice است، که به کاربران یک پلتفرم متمرکز برای fine-tuning آسان مدل‌ها برای نیازهای خاص ارائه می‌دهد. `;export{e as default};
